{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST 실습 (Pytorch)"
      ],
      "metadata": {
        "id": "jsIDAKfo6ha6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 환경 설정"
      ],
      "metadata": {
        "id": "02aQ79aA6ha7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jZtnC00b6ha7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*먼저, GPU 사용 여부를 결정합니다.*"
      ],
      "metadata": {
        "id": "5xOSFRuD6ha7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\" )\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "etOHzERc6ha7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ebe564-4484-4f4a-dc70-4d523e83f894"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 준비"
      ],
      "metadata": {
        "id": "M5KINtXQCQue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토치에서 random seed를 고정하기 위해 manual_seed()를 사용한다.\n",
        "- random seed를 고정한다는 말은 동일한 셋트의 난수를 생성할 수 있게 하는 것이다."
      ],
      "metadata": {
        "id": "XXZzjUD-6ha7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUjn6pxQ_bOQ",
        "outputId": "8d90ee2c-6fb9-4eea-ab12-a397f689cbcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d4d142ef9b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 불러오기 전에 데이터 전처리 과정을 미리 정의하기\n",
        "\n",
        "---\n",
        "\n",
        "torchvision.transforms 모듈은\n",
        "1. pre-processing\n",
        "2. data augmentation\n",
        "아래 링크 참고하기\n",
        "\n",
        "https://wikidocs.net/194919\n",
        "\n",
        "---\n",
        "\n",
        "ToTensor() 사용 이유\n",
        "\n",
        "1. 배열구조 변환\n",
        "이미지 또는 넘파이 배열은 HWC순이다.\n",
        "ToTensor()를 사용하면 토치의 배열 구조인 CHW로 변환해준다.\n",
        "\n",
        "2. 값의 크기를 조정(scaling)\n",
        "0-255 → 0-1\n"
      ],
      "metadata": {
        "id": "mNhQI0CRp_rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor() # 텐서로 변환\n",
        "])"
      ],
      "metadata": {
        "id": "D_a8NbUrpkNQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train, validation, test data를 설정합시다.\n",
        "\n",
        "---\n",
        "\n",
        "Dataset 은 샘플과 정답(label)을 저장하고\n",
        "dataset 클래스는 __len__()과 __getitem__()이 꼭 있어야 함!\n",
        "\n",
        "DataLoader 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다.\n",
        "- 이때 중요한 것은 batch 크기 데이터를 불러오는 걸 해준다.\n",
        "\n",
        "- shuffle = true : each epoch 마다 데이터 학습되는 순서가 변경된다.\n",
        "\n",
        "아래 링크 참고하기\n",
        "https://wikidocs.net/156998\n",
        "\n",
        "---\n",
        "torch.utils.data.random_split()함수\n",
        "- 데이터를 random 하게 split"
      ],
      "metadata": {
        "id": "nO31Orm76ha7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "test_batch_size = 1024\n",
        "\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.2\n",
        "\n",
        "mnist_train = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "len_mnist_train = len(mnist_train)\n",
        "train_size = int(len_mnist_train * train_ratio)\n",
        "validation_size = int(len_mnist_train * validation_ratio)\n",
        "\n",
        "train_dataset, validation_dataset = torch.utils.data.random_split(mnist_train, [train_size, validation_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=test_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "683pcSv06ha7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b389ebea-1d40-4fec-caf5-6ed5b07adbb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 125057786.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 48531928.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 185971049.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22897270.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*다운받은 data를 확인해보겠습니다.*"
      ],
      "metadata": {
        "id": "EuDgBNhl6ha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 셀을 계속해서 실행하면 next()함수로 인해 다음 데이터가 반환된다.\n",
        "# 그래서 출력 결과가 매번 다를 것이다.\n",
        "it_train = iter(train_loader)\n",
        "img, label = next(it_train) # train loader에서 하나씩 꺼내려면 iter를 사용하면 된다.\n",
        "#train x, train y\n",
        "\n",
        "print(img.shape) # torch.Size는 순서대로 ([batch_size, color, height, width])를 의미합니다.\n",
        "print(label)\n",
        "print(f\"배치 크기: {len(label)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c3f76c-994b-4ebd-f64c-af01f1b76a56",
        "id": "udrrzS2K6ha8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n",
            "tensor([7, 0, 0, 7, 8, 0, 2, 0, 8, 3, 8, 2, 2, 4, 7, 1, 2, 4, 2, 6, 5, 1, 9, 9,\n",
            "        8, 7, 6, 5, 0, 3, 3, 3])\n",
            "배치 크기: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*batch_size와 color 차원을 제거하고 이미지를 랜더링 해봅시다.*"
      ],
      "metadata": {
        "id": "QJLPSuCf6ha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "img_show = img[index, 0, :, :] # 암기하기     [ batchsize, channel, height, width ]\n",
        "plt.imshow(img_show, 'gray') # plt.imshow() 함수는 좌표평면 위에 이미지를 출력\n",
        "plt.show() # 이건 imshow() 다음에 꼭 사용하기\n",
        "print(label[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "6de1a1d1-3bd0-4f4f-fd6c-846f6049682f",
        "id": "-muaftjV6ha8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 모델 정의\n",
        "\n",
        "모델 1은\n",
        "- 레이어1: 16개의 노드\n",
        "- 레이어2: 10개의 노드"
      ],
      "metadata": {
        "id": "mlUp8Q8_Coju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이때 torch.nn.Module을 상속받아야 한다. (상속: 어떤 클래스를 만들 때 다른 클래스의 기능을 가져오는 것)\n",
        "\n",
        "이때 __init()__과 forward()를 override해야한다. (override: 부모 클래스인 torch.nn.Module에서 정의한 메소드를 자식 클래스에서 변경하는 것)\n",
        "- __init()__ : layer를 정의하는 곳\n",
        "- forward() : 모델에서 실행되는 계산을 정의. backward()는 토치에서 알아서 해준다!\n",
        "\n",
        "---\n",
        "\n",
        "forward() 함수\n",
        "- 보면, view() 함수를 사용한다. view는 reshape하는 함수이다. 링크를 참고해라! (https://jimmy-ai.tistory.com/151)\n",
        "- F.log_softmax()함수는 log(softmax)랑 같은 개념. softmax 사용 시 vanishing gradient 문제 때문에, 이걸 사용하는 경우가 많음."
      ],
      "metadata": {
        "id": "GPspXJgA49Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(torch.nn.Module): # 토치에서는 모델을 정의할 때 class로 만들어야한다.\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # Layer 정의\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "  # Layer 쌓기\n",
        "    def forward(self, x):\n",
        "        x = x.float() # x는 입력받는 이미지를 의미함\n",
        "        h1 = F.relu(self.fc1(x.view(-1, 784))) # import torch.nn.functional as F\n",
        "        h2 = F.relu(self.fc2(h1))\n",
        "        h3 = self.fc3(h2)\n",
        "        return F.log_softmax(h3, dim=1)"
      ],
      "metadata": {
        "id": "x_TzBWET6ha9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device) # to(device)로 모델 gpu에 보내기"
      ],
      "metadata": {
        "id": "Hz_4JfVm6ha-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 모델 학습"
      ],
      "metadata": {
        "id": "QlE2x-sxoGml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 enumerate의 실행 결과\n",
        "\n",
        "```\n",
        "list1 = [1, 2, 3, 4, 5, 6, 10]\n",
        "for idx, data in enumerate(list1):\n",
        "    print(idx, data)\n",
        "\n",
        "0 1\n",
        "1 2\n",
        "2 3\n",
        "3 4\n",
        "4 5\n",
        "5 6\n",
        "6 10\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pGfWQrgnpEE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*네트워크를 학습시키기 위해서 train 함수를 정의합니다.*"
      ],
      "metadata": {
        "id": "1NVwg9Zv6ha-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    flag = True\n",
        "    for batch_idx, (data, target) in enumerate(train_loader): # data를 하나씩 주는 게 아니라 batch 크기만큼 준다. 그래서 batch_idx를 사용하는 것이다.\n",
        "        data, target = data.to(device), target.to(device) # 데이터를 target device에 올려야 한다.\n",
        "        optimizer.zero_grad() # optimzer를 초기화\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target) # nll_loss(log_softmax) = cross entropy loss. 이미 forward 연산에서 log_softmax를 사용했으니깐, 요기서는 그냥 nll_loss만 사용해도 된다.\n",
        "        loss.backward() # backprop을 통한 gradient 계산\n",
        "        optimizer.step() # 실제 parameter의 값이 update되는 코드\n",
        "\n",
        "        if flag:\n",
        "          flag = False\n",
        "          print(target.shape, output.shape)\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "TDfXDYWn6ha-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*네트워크를 검증하기 위해서 validation 함수를 정의합니다.*\n",
        "\n",
        "---\n",
        "\n",
        "nll_loss()함수의 자세한 설명\n",
        "- nll_loss() 함수를 통해 계산된 loss는 item() 함수로 가져올 수 있다!\n",
        "- 이때 reduction='sum'을 사용하면, mini-batch안에 있는 모든 sample에 대한 loss를 sum하게 된다.\n",
        "- 만약, reduction='mean' (default setting)을 하면, batch안에 있는 모든 sample의 loss에 대한 평균을 구한다. 이건 training 때 사용되는 방법이다\n",
        "---\n",
        "argmax() 함수 실행 예시\n",
        "```\n",
        "#코드\n",
        "it = iter(train_loader)\n",
        "data, target = next(it)\n",
        "data, target = data.to(device), target.to(device)\n",
        "output = model(data)\n",
        "print(output[0])\n",
        "pred = output.argmax(dim=1, keepdim=True) # argmax를 사용하면 dimension 1 방향으로 max값의 idx가 return된다.\n",
        "print(pred[[0]])\n",
        "\n",
        "#결과\n",
        "tensor([-2.4081, -2.3636, -2.2304, -2.1932, -2.4621, -2.4544, -2.3904, -2.2540,\n",
        "        -2.2461, -2.0929], device='cuda:0', grad_fn=<SelectBackward0>)\n",
        "tensor([[9]], device='cuda:0')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SOyL39QhxAT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, device, validation_loader):\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): #gradient calculation을 하지 않아서 computation 절약 가능\n",
        "        for data, target in validation_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            validation_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True) # keepdim=True는 차원 유지하고 싶을 때 사용하는 코드인데, 굳이 알 필요 없다\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            # eq()함수는 각 텐서의 element-wise하게 비교하여 같다면 true 반환, 틀리면 false 반환\n",
        "            # target.view_as(pred)는 pred의 shape과 target이 같도록 reshape하는 방법\n",
        "            # 그 다음 sum, item은 일단 몰라도 된다. skip하자\n",
        "\n",
        "\n",
        "    validation_loss /= len(validation_loader.dataset) # dataset의 길이만큼 나누니깐 validation dataset 전체의 개수로 나누는 것이다. 즉, validation dataset loss의 평균을 구하는 식을 의미한다.\n",
        "\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
        "          (validation_loss, correct, len(validation_loader.dataset),\n",
        "        100. * correct / len(validation_loader.dataset)))"
      ],
      "metadata": {
        "id": "I3qsed7ww1e2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*네트워크의 학습 결과를 보기 위해서 test 함수를 정의합니다.*"
      ],
      "metadata": {
        "id": "nKX45eTT6ha-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): #gradient calculation을 하지 않아서 computation 절약 가능\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
        "          (test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "nLLVVbh66ha-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "lr = 0.01           # learning rate\n",
        "momentum = 0.5      # optimizer parameter\n",
        "log_interval = 200\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} # num of workers랑 pin memory는 나중에 공부하기. 지금은 몰라도 된다."
      ],
      "metadata": {
        "id": "g2oH_VqI6ha7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*위에서 정의한 함수를 사용하여 네트워크 학습을 진행 해봅시다.*"
      ],
      "metadata": {
        "id": "3z_sqcLg6ha-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
        "    validation(model, device, validation_loader)\n",
        "test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161ef958-d3a8-42e9-8a08-7d8805303e47",
        "id": "SEwkYP9n6ha_"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.313195\n",
            "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 2.005490\n",
            "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.905383\n",
            "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.391150\n",
            "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.589969\n",
            "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.611680\n",
            "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.531417\n",
            "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.323159\n",
            "\n",
            "Validation set: Average loss: 0.3532, Accuracy: 10778/12000 (90%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.191018\n",
            "Train Epoch: 2 [6400/48000 (13%)]\tLoss: 0.346260\n",
            "Train Epoch: 2 [12800/48000 (27%)]\tLoss: 0.404264\n",
            "Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.491201\n",
            "Train Epoch: 2 [25600/48000 (53%)]\tLoss: 0.210471\n",
            "Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.173744\n",
            "Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.223837\n",
            "Train Epoch: 2 [44800/48000 (93%)]\tLoss: 0.196701\n",
            "\n",
            "Validation set: Average loss: 0.2825, Accuracy: 11015/12000 (92%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.345872\n",
            "Train Epoch: 3 [6400/48000 (13%)]\tLoss: 0.441912\n",
            "Train Epoch: 3 [12800/48000 (27%)]\tLoss: 0.279895\n",
            "Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.156235\n",
            "Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.177881\n",
            "Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.360239\n",
            "Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.284761\n",
            "Train Epoch: 3 [44800/48000 (93%)]\tLoss: 0.132435\n",
            "\n",
            "Validation set: Average loss: 0.2234, Accuracy: 11219/12000 (93%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 4 [0/48000 (0%)]\tLoss: 0.126662\n",
            "Train Epoch: 4 [6400/48000 (13%)]\tLoss: 0.118594\n",
            "Train Epoch: 4 [12800/48000 (27%)]\tLoss: 0.122723\n",
            "Train Epoch: 4 [19200/48000 (40%)]\tLoss: 0.273212\n",
            "Train Epoch: 4 [25600/48000 (53%)]\tLoss: 0.200272\n",
            "Train Epoch: 4 [32000/48000 (67%)]\tLoss: 0.200154\n",
            "Train Epoch: 4 [38400/48000 (80%)]\tLoss: 0.121068\n",
            "Train Epoch: 4 [44800/48000 (93%)]\tLoss: 0.387772\n",
            "\n",
            "Validation set: Average loss: 0.1964, Accuracy: 11303/12000 (94%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 5 [0/48000 (0%)]\tLoss: 0.234134\n",
            "Train Epoch: 5 [6400/48000 (13%)]\tLoss: 0.161383\n",
            "Train Epoch: 5 [12800/48000 (27%)]\tLoss: 0.200943\n",
            "Train Epoch: 5 [19200/48000 (40%)]\tLoss: 0.538253\n",
            "Train Epoch: 5 [25600/48000 (53%)]\tLoss: 0.129424\n",
            "Train Epoch: 5 [32000/48000 (67%)]\tLoss: 0.082867\n",
            "Train Epoch: 5 [38400/48000 (80%)]\tLoss: 0.128378\n",
            "Train Epoch: 5 [44800/48000 (93%)]\tLoss: 0.239684\n",
            "\n",
            "Validation set: Average loss: 0.1703, Accuracy: 11400/12000 (95%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 6 [0/48000 (0%)]\tLoss: 0.103901\n",
            "Train Epoch: 6 [6400/48000 (13%)]\tLoss: 0.097345\n",
            "Train Epoch: 6 [12800/48000 (27%)]\tLoss: 0.066743\n",
            "Train Epoch: 6 [19200/48000 (40%)]\tLoss: 0.060514\n",
            "Train Epoch: 6 [25600/48000 (53%)]\tLoss: 0.152948\n",
            "Train Epoch: 6 [32000/48000 (67%)]\tLoss: 0.193625\n",
            "Train Epoch: 6 [38400/48000 (80%)]\tLoss: 0.035561\n",
            "Train Epoch: 6 [44800/48000 (93%)]\tLoss: 0.032635\n",
            "\n",
            "Validation set: Average loss: 0.1513, Accuracy: 11465/12000 (96%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 7 [0/48000 (0%)]\tLoss: 0.147325\n",
            "Train Epoch: 7 [6400/48000 (13%)]\tLoss: 0.094943\n",
            "Train Epoch: 7 [12800/48000 (27%)]\tLoss: 0.084234\n",
            "Train Epoch: 7 [19200/48000 (40%)]\tLoss: 0.075007\n",
            "Train Epoch: 7 [25600/48000 (53%)]\tLoss: 0.035401\n",
            "Train Epoch: 7 [32000/48000 (67%)]\tLoss: 0.306588\n",
            "Train Epoch: 7 [38400/48000 (80%)]\tLoss: 0.108773\n",
            "Train Epoch: 7 [44800/48000 (93%)]\tLoss: 0.078382\n",
            "\n",
            "Validation set: Average loss: 0.1400, Accuracy: 11496/12000 (96%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 8 [0/48000 (0%)]\tLoss: 0.153732\n",
            "Train Epoch: 8 [6400/48000 (13%)]\tLoss: 0.044417\n",
            "Train Epoch: 8 [12800/48000 (27%)]\tLoss: 0.204695\n",
            "Train Epoch: 8 [19200/48000 (40%)]\tLoss: 0.094433\n",
            "Train Epoch: 8 [25600/48000 (53%)]\tLoss: 0.152040\n",
            "Train Epoch: 8 [32000/48000 (67%)]\tLoss: 0.047076\n",
            "Train Epoch: 8 [38400/48000 (80%)]\tLoss: 0.097884\n",
            "Train Epoch: 8 [44800/48000 (93%)]\tLoss: 0.042665\n",
            "\n",
            "Validation set: Average loss: 0.1249, Accuracy: 11551/12000 (96%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 9 [0/48000 (0%)]\tLoss: 0.075154\n",
            "Train Epoch: 9 [6400/48000 (13%)]\tLoss: 0.047834\n",
            "Train Epoch: 9 [12800/48000 (27%)]\tLoss: 0.170076\n",
            "Train Epoch: 9 [19200/48000 (40%)]\tLoss: 0.012144\n",
            "Train Epoch: 9 [25600/48000 (53%)]\tLoss: 0.182683\n",
            "Train Epoch: 9 [32000/48000 (67%)]\tLoss: 0.046272\n",
            "Train Epoch: 9 [38400/48000 (80%)]\tLoss: 0.015296\n",
            "Train Epoch: 9 [44800/48000 (93%)]\tLoss: 0.154329\n",
            "\n",
            "Validation set: Average loss: 0.1164, Accuracy: 11574/12000 (96%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 10 [0/48000 (0%)]\tLoss: 0.094906\n",
            "Train Epoch: 10 [6400/48000 (13%)]\tLoss: 0.059813\n",
            "Train Epoch: 10 [12800/48000 (27%)]\tLoss: 0.030355\n",
            "Train Epoch: 10 [19200/48000 (40%)]\tLoss: 0.091449\n",
            "Train Epoch: 10 [25600/48000 (53%)]\tLoss: 0.017848\n",
            "Train Epoch: 10 [32000/48000 (67%)]\tLoss: 0.047546\n",
            "Train Epoch: 10 [38400/48000 (80%)]\tLoss: 0.137403\n",
            "Train Epoch: 10 [44800/48000 (93%)]\tLoss: 0.090724\n",
            "\n",
            "Validation set: Average loss: 0.1092, Accuracy: 11603/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 11 [0/48000 (0%)]\tLoss: 0.062196\n",
            "Train Epoch: 11 [6400/48000 (13%)]\tLoss: 0.132555\n",
            "Train Epoch: 11 [12800/48000 (27%)]\tLoss: 0.066158\n",
            "Train Epoch: 11 [19200/48000 (40%)]\tLoss: 0.184197\n",
            "Train Epoch: 11 [25600/48000 (53%)]\tLoss: 0.031883\n",
            "Train Epoch: 11 [32000/48000 (67%)]\tLoss: 0.036769\n",
            "Train Epoch: 11 [38400/48000 (80%)]\tLoss: 0.074797\n",
            "Train Epoch: 11 [44800/48000 (93%)]\tLoss: 0.049436\n",
            "\n",
            "Validation set: Average loss: 0.1063, Accuracy: 11618/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 12 [0/48000 (0%)]\tLoss: 0.093016\n",
            "Train Epoch: 12 [6400/48000 (13%)]\tLoss: 0.075989\n",
            "Train Epoch: 12 [12800/48000 (27%)]\tLoss: 0.094810\n",
            "Train Epoch: 12 [19200/48000 (40%)]\tLoss: 0.041642\n",
            "Train Epoch: 12 [25600/48000 (53%)]\tLoss: 0.019331\n",
            "Train Epoch: 12 [32000/48000 (67%)]\tLoss: 0.154767\n",
            "Train Epoch: 12 [38400/48000 (80%)]\tLoss: 0.048869\n",
            "Train Epoch: 12 [44800/48000 (93%)]\tLoss: 0.025194\n",
            "\n",
            "Validation set: Average loss: 0.1013, Accuracy: 11615/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 13 [0/48000 (0%)]\tLoss: 0.081640\n",
            "Train Epoch: 13 [6400/48000 (13%)]\tLoss: 0.031425\n",
            "Train Epoch: 13 [12800/48000 (27%)]\tLoss: 0.027111\n",
            "Train Epoch: 13 [19200/48000 (40%)]\tLoss: 0.016777\n",
            "Train Epoch: 13 [25600/48000 (53%)]\tLoss: 0.062071\n",
            "Train Epoch: 13 [32000/48000 (67%)]\tLoss: 0.010530\n",
            "Train Epoch: 13 [38400/48000 (80%)]\tLoss: 0.362044\n",
            "Train Epoch: 13 [44800/48000 (93%)]\tLoss: 0.069727\n",
            "\n",
            "Validation set: Average loss: 0.0964, Accuracy: 11639/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 14 [0/48000 (0%)]\tLoss: 0.013903\n",
            "Train Epoch: 14 [6400/48000 (13%)]\tLoss: 0.054377\n",
            "Train Epoch: 14 [12800/48000 (27%)]\tLoss: 0.027494\n",
            "Train Epoch: 14 [19200/48000 (40%)]\tLoss: 0.014774\n",
            "Train Epoch: 14 [25600/48000 (53%)]\tLoss: 0.031507\n",
            "Train Epoch: 14 [32000/48000 (67%)]\tLoss: 0.022929\n",
            "Train Epoch: 14 [38400/48000 (80%)]\tLoss: 0.033350\n",
            "Train Epoch: 14 [44800/48000 (93%)]\tLoss: 0.052321\n",
            "\n",
            "Validation set: Average loss: 0.0907, Accuracy: 11682/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 15 [0/48000 (0%)]\tLoss: 0.008498\n",
            "Train Epoch: 15 [6400/48000 (13%)]\tLoss: 0.025284\n",
            "Train Epoch: 15 [12800/48000 (27%)]\tLoss: 0.012996\n",
            "Train Epoch: 15 [19200/48000 (40%)]\tLoss: 0.018204\n",
            "Train Epoch: 15 [25600/48000 (53%)]\tLoss: 0.027345\n",
            "Train Epoch: 15 [32000/48000 (67%)]\tLoss: 0.046581\n",
            "Train Epoch: 15 [38400/48000 (80%)]\tLoss: 0.007913\n",
            "Train Epoch: 15 [44800/48000 (93%)]\tLoss: 0.019241\n",
            "\n",
            "Validation set: Average loss: 0.0887, Accuracy: 11687/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 16 [0/48000 (0%)]\tLoss: 0.055143\n",
            "Train Epoch: 16 [6400/48000 (13%)]\tLoss: 0.028384\n",
            "Train Epoch: 16 [12800/48000 (27%)]\tLoss: 0.050945\n",
            "Train Epoch: 16 [19200/48000 (40%)]\tLoss: 0.030553\n",
            "Train Epoch: 16 [25600/48000 (53%)]\tLoss: 0.025210\n",
            "Train Epoch: 16 [32000/48000 (67%)]\tLoss: 0.040282\n",
            "Train Epoch: 16 [38400/48000 (80%)]\tLoss: 0.014379\n",
            "Train Epoch: 16 [44800/48000 (93%)]\tLoss: 0.027613\n",
            "\n",
            "Validation set: Average loss: 0.0875, Accuracy: 11676/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 17 [0/48000 (0%)]\tLoss: 0.023127\n",
            "Train Epoch: 17 [6400/48000 (13%)]\tLoss: 0.106534\n",
            "Train Epoch: 17 [12800/48000 (27%)]\tLoss: 0.019500\n",
            "Train Epoch: 17 [19200/48000 (40%)]\tLoss: 0.134919\n",
            "Train Epoch: 17 [25600/48000 (53%)]\tLoss: 0.005788\n",
            "Train Epoch: 17 [32000/48000 (67%)]\tLoss: 0.032445\n",
            "Train Epoch: 17 [38400/48000 (80%)]\tLoss: 0.031203\n",
            "Train Epoch: 17 [44800/48000 (93%)]\tLoss: 0.085077\n",
            "\n",
            "Validation set: Average loss: 0.0936, Accuracy: 11672/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 18 [0/48000 (0%)]\tLoss: 0.014835\n",
            "Train Epoch: 18 [6400/48000 (13%)]\tLoss: 0.054270\n",
            "Train Epoch: 18 [12800/48000 (27%)]\tLoss: 0.002459\n",
            "Train Epoch: 18 [19200/48000 (40%)]\tLoss: 0.005235\n",
            "Train Epoch: 18 [25600/48000 (53%)]\tLoss: 0.110418\n",
            "Train Epoch: 18 [32000/48000 (67%)]\tLoss: 0.046770\n",
            "Train Epoch: 18 [38400/48000 (80%)]\tLoss: 0.007230\n",
            "Train Epoch: 18 [44800/48000 (93%)]\tLoss: 0.032684\n",
            "\n",
            "Validation set: Average loss: 0.0817, Accuracy: 11718/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 19 [0/48000 (0%)]\tLoss: 0.066716\n",
            "Train Epoch: 19 [6400/48000 (13%)]\tLoss: 0.013583\n",
            "Train Epoch: 19 [12800/48000 (27%)]\tLoss: 0.039270\n",
            "Train Epoch: 19 [19200/48000 (40%)]\tLoss: 0.106586\n",
            "Train Epoch: 19 [25600/48000 (53%)]\tLoss: 0.061999\n",
            "Train Epoch: 19 [32000/48000 (67%)]\tLoss: 0.009298\n",
            "Train Epoch: 19 [38400/48000 (80%)]\tLoss: 0.016624\n",
            "Train Epoch: 19 [44800/48000 (93%)]\tLoss: 0.003629\n",
            "\n",
            "Validation set: Average loss: 0.0835, Accuracy: 11693/12000 (97%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 20 [0/48000 (0%)]\tLoss: 0.029348\n",
            "Train Epoch: 20 [6400/48000 (13%)]\tLoss: 0.006881\n",
            "Train Epoch: 20 [12800/48000 (27%)]\tLoss: 0.019806\n",
            "Train Epoch: 20 [19200/48000 (40%)]\tLoss: 0.004529\n",
            "Train Epoch: 20 [25600/48000 (53%)]\tLoss: 0.127381\n",
            "Train Epoch: 20 [32000/48000 (67%)]\tLoss: 0.003732\n",
            "Train Epoch: 20 [38400/48000 (80%)]\tLoss: 0.107829\n",
            "Train Epoch: 20 [44800/48000 (93%)]\tLoss: 0.038816\n",
            "\n",
            "Validation set: Average loss: 0.0837, Accuracy: 11706/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 21 [0/48000 (0%)]\tLoss: 0.006019\n",
            "Train Epoch: 21 [6400/48000 (13%)]\tLoss: 0.006177\n",
            "Train Epoch: 21 [12800/48000 (27%)]\tLoss: 0.008681\n",
            "Train Epoch: 21 [19200/48000 (40%)]\tLoss: 0.012897\n",
            "Train Epoch: 21 [25600/48000 (53%)]\tLoss: 0.023086\n",
            "Train Epoch: 21 [32000/48000 (67%)]\tLoss: 0.020109\n",
            "Train Epoch: 21 [38400/48000 (80%)]\tLoss: 0.022548\n",
            "Train Epoch: 21 [44800/48000 (93%)]\tLoss: 0.102145\n",
            "\n",
            "Validation set: Average loss: 0.0828, Accuracy: 11703/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 22 [0/48000 (0%)]\tLoss: 0.036459\n",
            "Train Epoch: 22 [6400/48000 (13%)]\tLoss: 0.015870\n",
            "Train Epoch: 22 [12800/48000 (27%)]\tLoss: 0.009822\n",
            "Train Epoch: 22 [19200/48000 (40%)]\tLoss: 0.033548\n",
            "Train Epoch: 22 [25600/48000 (53%)]\tLoss: 0.006049\n",
            "Train Epoch: 22 [32000/48000 (67%)]\tLoss: 0.003095\n",
            "Train Epoch: 22 [38400/48000 (80%)]\tLoss: 0.014764\n",
            "Train Epoch: 22 [44800/48000 (93%)]\tLoss: 0.027964\n",
            "\n",
            "Validation set: Average loss: 0.0803, Accuracy: 11705/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 23 [0/48000 (0%)]\tLoss: 0.048683\n",
            "Train Epoch: 23 [6400/48000 (13%)]\tLoss: 0.024565\n",
            "Train Epoch: 23 [12800/48000 (27%)]\tLoss: 0.046736\n",
            "Train Epoch: 23 [19200/48000 (40%)]\tLoss: 0.034649\n",
            "Train Epoch: 23 [25600/48000 (53%)]\tLoss: 0.005493\n",
            "Train Epoch: 23 [32000/48000 (67%)]\tLoss: 0.021044\n",
            "Train Epoch: 23 [38400/48000 (80%)]\tLoss: 0.054150\n",
            "Train Epoch: 23 [44800/48000 (93%)]\tLoss: 0.015815\n",
            "\n",
            "Validation set: Average loss: 0.0796, Accuracy: 11708/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 24 [0/48000 (0%)]\tLoss: 0.002882\n",
            "Train Epoch: 24 [6400/48000 (13%)]\tLoss: 0.016825\n",
            "Train Epoch: 24 [12800/48000 (27%)]\tLoss: 0.021028\n",
            "Train Epoch: 24 [19200/48000 (40%)]\tLoss: 0.077193\n",
            "Train Epoch: 24 [25600/48000 (53%)]\tLoss: 0.056274\n",
            "Train Epoch: 24 [32000/48000 (67%)]\tLoss: 0.019388\n",
            "Train Epoch: 24 [38400/48000 (80%)]\tLoss: 0.003967\n",
            "Train Epoch: 24 [44800/48000 (93%)]\tLoss: 0.011137\n",
            "\n",
            "Validation set: Average loss: 0.0780, Accuracy: 11715/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 25 [0/48000 (0%)]\tLoss: 0.009406\n",
            "Train Epoch: 25 [6400/48000 (13%)]\tLoss: 0.007904\n",
            "Train Epoch: 25 [12800/48000 (27%)]\tLoss: 0.005453\n",
            "Train Epoch: 25 [19200/48000 (40%)]\tLoss: 0.039610\n",
            "Train Epoch: 25 [25600/48000 (53%)]\tLoss: 0.011822\n",
            "Train Epoch: 25 [32000/48000 (67%)]\tLoss: 0.026069\n",
            "Train Epoch: 25 [38400/48000 (80%)]\tLoss: 0.010724\n",
            "Train Epoch: 25 [44800/48000 (93%)]\tLoss: 0.014978\n",
            "\n",
            "Validation set: Average loss: 0.0790, Accuracy: 11714/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 26 [0/48000 (0%)]\tLoss: 0.060252\n",
            "Train Epoch: 26 [6400/48000 (13%)]\tLoss: 0.013173\n",
            "Train Epoch: 26 [12800/48000 (27%)]\tLoss: 0.019240\n",
            "Train Epoch: 26 [19200/48000 (40%)]\tLoss: 0.007818\n",
            "Train Epoch: 26 [25600/48000 (53%)]\tLoss: 0.016415\n",
            "Train Epoch: 26 [32000/48000 (67%)]\tLoss: 0.008869\n",
            "Train Epoch: 26 [38400/48000 (80%)]\tLoss: 0.015141\n",
            "Train Epoch: 26 [44800/48000 (93%)]\tLoss: 0.013473\n",
            "\n",
            "Validation set: Average loss: 0.0784, Accuracy: 11709/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 27 [0/48000 (0%)]\tLoss: 0.013542\n",
            "Train Epoch: 27 [6400/48000 (13%)]\tLoss: 0.009472\n",
            "Train Epoch: 27 [12800/48000 (27%)]\tLoss: 0.025586\n",
            "Train Epoch: 27 [19200/48000 (40%)]\tLoss: 0.019658\n",
            "Train Epoch: 27 [25600/48000 (53%)]\tLoss: 0.009806\n",
            "Train Epoch: 27 [32000/48000 (67%)]\tLoss: 0.001646\n",
            "Train Epoch: 27 [38400/48000 (80%)]\tLoss: 0.128607\n",
            "Train Epoch: 27 [44800/48000 (93%)]\tLoss: 0.020861\n",
            "\n",
            "Validation set: Average loss: 0.0787, Accuracy: 11723/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 28 [0/48000 (0%)]\tLoss: 0.004607\n",
            "Train Epoch: 28 [6400/48000 (13%)]\tLoss: 0.002549\n",
            "Train Epoch: 28 [12800/48000 (27%)]\tLoss: 0.013067\n",
            "Train Epoch: 28 [19200/48000 (40%)]\tLoss: 0.005102\n",
            "Train Epoch: 28 [25600/48000 (53%)]\tLoss: 0.001293\n",
            "Train Epoch: 28 [32000/48000 (67%)]\tLoss: 0.028211\n",
            "Train Epoch: 28 [38400/48000 (80%)]\tLoss: 0.009048\n",
            "Train Epoch: 28 [44800/48000 (93%)]\tLoss: 0.002013\n",
            "\n",
            "Validation set: Average loss: 0.0784, Accuracy: 11729/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 29 [0/48000 (0%)]\tLoss: 0.001892\n",
            "Train Epoch: 29 [6400/48000 (13%)]\tLoss: 0.004026\n",
            "Train Epoch: 29 [12800/48000 (27%)]\tLoss: 0.006787\n",
            "Train Epoch: 29 [19200/48000 (40%)]\tLoss: 0.030182\n",
            "Train Epoch: 29 [25600/48000 (53%)]\tLoss: 0.009713\n",
            "Train Epoch: 29 [32000/48000 (67%)]\tLoss: 0.011638\n",
            "Train Epoch: 29 [38400/48000 (80%)]\tLoss: 0.001944\n",
            "Train Epoch: 29 [44800/48000 (93%)]\tLoss: 0.006400\n",
            "\n",
            "Validation set: Average loss: 0.0780, Accuracy: 11723/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 30 [0/48000 (0%)]\tLoss: 0.009575\n",
            "Train Epoch: 30 [6400/48000 (13%)]\tLoss: 0.001043\n",
            "Train Epoch: 30 [12800/48000 (27%)]\tLoss: 0.005876\n",
            "Train Epoch: 30 [19200/48000 (40%)]\tLoss: 0.008199\n",
            "Train Epoch: 30 [25600/48000 (53%)]\tLoss: 0.001353\n",
            "Train Epoch: 30 [32000/48000 (67%)]\tLoss: 0.009053\n",
            "Train Epoch: 30 [38400/48000 (80%)]\tLoss: 0.003263\n",
            "Train Epoch: 30 [44800/48000 (93%)]\tLoss: 0.006356\n",
            "\n",
            "Validation set: Average loss: 0.0770, Accuracy: 11735/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 31 [0/48000 (0%)]\tLoss: 0.008558\n",
            "Train Epoch: 31 [6400/48000 (13%)]\tLoss: 0.011292\n",
            "Train Epoch: 31 [12800/48000 (27%)]\tLoss: 0.067098\n",
            "Train Epoch: 31 [19200/48000 (40%)]\tLoss: 0.049214\n",
            "Train Epoch: 31 [25600/48000 (53%)]\tLoss: 0.015958\n",
            "Train Epoch: 31 [32000/48000 (67%)]\tLoss: 0.002177\n",
            "Train Epoch: 31 [38400/48000 (80%)]\tLoss: 0.024018\n",
            "Train Epoch: 31 [44800/48000 (93%)]\tLoss: 0.006937\n",
            "\n",
            "Validation set: Average loss: 0.0786, Accuracy: 11724/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 32 [0/48000 (0%)]\tLoss: 0.003242\n",
            "Train Epoch: 32 [6400/48000 (13%)]\tLoss: 0.007070\n",
            "Train Epoch: 32 [12800/48000 (27%)]\tLoss: 0.004274\n",
            "Train Epoch: 32 [19200/48000 (40%)]\tLoss: 0.003690\n",
            "Train Epoch: 32 [25600/48000 (53%)]\tLoss: 0.004509\n",
            "Train Epoch: 32 [32000/48000 (67%)]\tLoss: 0.006445\n",
            "Train Epoch: 32 [38400/48000 (80%)]\tLoss: 0.029215\n",
            "Train Epoch: 32 [44800/48000 (93%)]\tLoss: 0.017264\n",
            "\n",
            "Validation set: Average loss: 0.0764, Accuracy: 11742/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 33 [0/48000 (0%)]\tLoss: 0.004359\n",
            "Train Epoch: 33 [6400/48000 (13%)]\tLoss: 0.007575\n",
            "Train Epoch: 33 [12800/48000 (27%)]\tLoss: 0.013474\n",
            "Train Epoch: 33 [19200/48000 (40%)]\tLoss: 0.002204\n",
            "Train Epoch: 33 [25600/48000 (53%)]\tLoss: 0.004269\n",
            "Train Epoch: 33 [32000/48000 (67%)]\tLoss: 0.005253\n",
            "Train Epoch: 33 [38400/48000 (80%)]\tLoss: 0.007591\n",
            "Train Epoch: 33 [44800/48000 (93%)]\tLoss: 0.015642\n",
            "\n",
            "Validation set: Average loss: 0.0825, Accuracy: 11722/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 34 [0/48000 (0%)]\tLoss: 0.009417\n",
            "Train Epoch: 34 [6400/48000 (13%)]\tLoss: 0.005848\n",
            "Train Epoch: 34 [12800/48000 (27%)]\tLoss: 0.005419\n",
            "Train Epoch: 34 [19200/48000 (40%)]\tLoss: 0.006700\n",
            "Train Epoch: 34 [25600/48000 (53%)]\tLoss: 0.020793\n",
            "Train Epoch: 34 [32000/48000 (67%)]\tLoss: 0.002087\n",
            "Train Epoch: 34 [38400/48000 (80%)]\tLoss: 0.026936\n",
            "Train Epoch: 34 [44800/48000 (93%)]\tLoss: 0.000733\n",
            "\n",
            "Validation set: Average loss: 0.0807, Accuracy: 11715/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 35 [0/48000 (0%)]\tLoss: 0.001554\n",
            "Train Epoch: 35 [6400/48000 (13%)]\tLoss: 0.008285\n",
            "Train Epoch: 35 [12800/48000 (27%)]\tLoss: 0.014410\n",
            "Train Epoch: 35 [19200/48000 (40%)]\tLoss: 0.003049\n",
            "Train Epoch: 35 [25600/48000 (53%)]\tLoss: 0.002984\n",
            "Train Epoch: 35 [32000/48000 (67%)]\tLoss: 0.006760\n",
            "Train Epoch: 35 [38400/48000 (80%)]\tLoss: 0.025513\n",
            "Train Epoch: 35 [44800/48000 (93%)]\tLoss: 0.016944\n",
            "\n",
            "Validation set: Average loss: 0.0803, Accuracy: 11722/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 36 [0/48000 (0%)]\tLoss: 0.013060\n",
            "Train Epoch: 36 [6400/48000 (13%)]\tLoss: 0.001382\n",
            "Train Epoch: 36 [12800/48000 (27%)]\tLoss: 0.000950\n",
            "Train Epoch: 36 [19200/48000 (40%)]\tLoss: 0.004676\n",
            "Train Epoch: 36 [25600/48000 (53%)]\tLoss: 0.010002\n",
            "Train Epoch: 36 [32000/48000 (67%)]\tLoss: 0.015273\n",
            "Train Epoch: 36 [38400/48000 (80%)]\tLoss: 0.001084\n",
            "Train Epoch: 36 [44800/48000 (93%)]\tLoss: 0.010649\n",
            "\n",
            "Validation set: Average loss: 0.0776, Accuracy: 11740/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 37 [0/48000 (0%)]\tLoss: 0.009174\n",
            "Train Epoch: 37 [6400/48000 (13%)]\tLoss: 0.002531\n",
            "Train Epoch: 37 [12800/48000 (27%)]\tLoss: 0.003685\n",
            "Train Epoch: 37 [19200/48000 (40%)]\tLoss: 0.002669\n",
            "Train Epoch: 37 [25600/48000 (53%)]\tLoss: 0.001788\n",
            "Train Epoch: 37 [32000/48000 (67%)]\tLoss: 0.008678\n",
            "Train Epoch: 37 [38400/48000 (80%)]\tLoss: 0.006825\n",
            "Train Epoch: 37 [44800/48000 (93%)]\tLoss: 0.003569\n",
            "\n",
            "Validation set: Average loss: 0.0795, Accuracy: 11728/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 38 [0/48000 (0%)]\tLoss: 0.018229\n",
            "Train Epoch: 38 [6400/48000 (13%)]\tLoss: 0.014640\n",
            "Train Epoch: 38 [12800/48000 (27%)]\tLoss: 0.005257\n",
            "Train Epoch: 38 [19200/48000 (40%)]\tLoss: 0.004863\n",
            "Train Epoch: 38 [25600/48000 (53%)]\tLoss: 0.001101\n",
            "Train Epoch: 38 [32000/48000 (67%)]\tLoss: 0.009717\n",
            "Train Epoch: 38 [38400/48000 (80%)]\tLoss: 0.010469\n",
            "Train Epoch: 38 [44800/48000 (93%)]\tLoss: 0.001528\n",
            "\n",
            "Validation set: Average loss: 0.0811, Accuracy: 11726/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 39 [0/48000 (0%)]\tLoss: 0.007504\n",
            "Train Epoch: 39 [6400/48000 (13%)]\tLoss: 0.000824\n",
            "Train Epoch: 39 [12800/48000 (27%)]\tLoss: 0.006231\n",
            "Train Epoch: 39 [19200/48000 (40%)]\tLoss: 0.007392\n",
            "Train Epoch: 39 [25600/48000 (53%)]\tLoss: 0.003452\n",
            "Train Epoch: 39 [32000/48000 (67%)]\tLoss: 0.004993\n",
            "Train Epoch: 39 [38400/48000 (80%)]\tLoss: 0.007468\n",
            "Train Epoch: 39 [44800/48000 (93%)]\tLoss: 0.011673\n",
            "\n",
            "Validation set: Average loss: 0.0798, Accuracy: 11736/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 40 [0/48000 (0%)]\tLoss: 0.004808\n",
            "Train Epoch: 40 [6400/48000 (13%)]\tLoss: 0.001996\n",
            "Train Epoch: 40 [12800/48000 (27%)]\tLoss: 0.000615\n",
            "Train Epoch: 40 [19200/48000 (40%)]\tLoss: 0.018207\n",
            "Train Epoch: 40 [25600/48000 (53%)]\tLoss: 0.001525\n",
            "Train Epoch: 40 [32000/48000 (67%)]\tLoss: 0.007765\n",
            "Train Epoch: 40 [38400/48000 (80%)]\tLoss: 0.004605\n",
            "Train Epoch: 40 [44800/48000 (93%)]\tLoss: 0.001812\n",
            "\n",
            "Validation set: Average loss: 0.0812, Accuracy: 11736/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 41 [0/48000 (0%)]\tLoss: 0.000907\n",
            "Train Epoch: 41 [6400/48000 (13%)]\tLoss: 0.001102\n",
            "Train Epoch: 41 [12800/48000 (27%)]\tLoss: 0.001794\n",
            "Train Epoch: 41 [19200/48000 (40%)]\tLoss: 0.014129\n",
            "Train Epoch: 41 [25600/48000 (53%)]\tLoss: 0.011554\n",
            "Train Epoch: 41 [32000/48000 (67%)]\tLoss: 0.004640\n",
            "Train Epoch: 41 [38400/48000 (80%)]\tLoss: 0.005027\n",
            "Train Epoch: 41 [44800/48000 (93%)]\tLoss: 0.006587\n",
            "\n",
            "Validation set: Average loss: 0.0818, Accuracy: 11734/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 42 [0/48000 (0%)]\tLoss: 0.005893\n",
            "Train Epoch: 42 [6400/48000 (13%)]\tLoss: 0.000844\n",
            "Train Epoch: 42 [12800/48000 (27%)]\tLoss: 0.018014\n",
            "Train Epoch: 42 [19200/48000 (40%)]\tLoss: 0.012831\n",
            "Train Epoch: 42 [25600/48000 (53%)]\tLoss: 0.001784\n",
            "Train Epoch: 42 [32000/48000 (67%)]\tLoss: 0.007166\n",
            "Train Epoch: 42 [38400/48000 (80%)]\tLoss: 0.007624\n",
            "Train Epoch: 42 [44800/48000 (93%)]\tLoss: 0.001138\n",
            "\n",
            "Validation set: Average loss: 0.0804, Accuracy: 11740/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 43 [0/48000 (0%)]\tLoss: 0.001415\n",
            "Train Epoch: 43 [6400/48000 (13%)]\tLoss: 0.000080\n",
            "Train Epoch: 43 [12800/48000 (27%)]\tLoss: 0.007189\n",
            "Train Epoch: 43 [19200/48000 (40%)]\tLoss: 0.002262\n",
            "Train Epoch: 43 [25600/48000 (53%)]\tLoss: 0.000822\n",
            "Train Epoch: 43 [32000/48000 (67%)]\tLoss: 0.001992\n",
            "Train Epoch: 43 [38400/48000 (80%)]\tLoss: 0.000233\n",
            "Train Epoch: 43 [44800/48000 (93%)]\tLoss: 0.003224\n",
            "\n",
            "Validation set: Average loss: 0.0810, Accuracy: 11739/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 44 [0/48000 (0%)]\tLoss: 0.002672\n",
            "Train Epoch: 44 [6400/48000 (13%)]\tLoss: 0.009466\n",
            "Train Epoch: 44 [12800/48000 (27%)]\tLoss: 0.009087\n",
            "Train Epoch: 44 [19200/48000 (40%)]\tLoss: 0.001713\n",
            "Train Epoch: 44 [25600/48000 (53%)]\tLoss: 0.001325\n",
            "Train Epoch: 44 [32000/48000 (67%)]\tLoss: 0.001561\n",
            "Train Epoch: 44 [38400/48000 (80%)]\tLoss: 0.003458\n",
            "Train Epoch: 44 [44800/48000 (93%)]\tLoss: 0.001022\n",
            "\n",
            "Validation set: Average loss: 0.0823, Accuracy: 11734/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 45 [0/48000 (0%)]\tLoss: 0.003821\n",
            "Train Epoch: 45 [6400/48000 (13%)]\tLoss: 0.005445\n",
            "Train Epoch: 45 [12800/48000 (27%)]\tLoss: 0.006408\n",
            "Train Epoch: 45 [19200/48000 (40%)]\tLoss: 0.001316\n",
            "Train Epoch: 45 [25600/48000 (53%)]\tLoss: 0.002007\n",
            "Train Epoch: 45 [32000/48000 (67%)]\tLoss: 0.001926\n",
            "Train Epoch: 45 [38400/48000 (80%)]\tLoss: 0.001181\n",
            "Train Epoch: 45 [44800/48000 (93%)]\tLoss: 0.002222\n",
            "\n",
            "Validation set: Average loss: 0.0823, Accuracy: 11738/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 46 [0/48000 (0%)]\tLoss: 0.005355\n",
            "Train Epoch: 46 [6400/48000 (13%)]\tLoss: 0.000656\n",
            "Train Epoch: 46 [12800/48000 (27%)]\tLoss: 0.000056\n",
            "Train Epoch: 46 [19200/48000 (40%)]\tLoss: 0.006355\n",
            "Train Epoch: 46 [25600/48000 (53%)]\tLoss: 0.002249\n",
            "Train Epoch: 46 [32000/48000 (67%)]\tLoss: 0.004540\n",
            "Train Epoch: 46 [38400/48000 (80%)]\tLoss: 0.001581\n",
            "Train Epoch: 46 [44800/48000 (93%)]\tLoss: 0.002129\n",
            "\n",
            "Validation set: Average loss: 0.0832, Accuracy: 11742/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 47 [0/48000 (0%)]\tLoss: 0.001143\n",
            "Train Epoch: 47 [6400/48000 (13%)]\tLoss: 0.009083\n",
            "Train Epoch: 47 [12800/48000 (27%)]\tLoss: 0.003675\n",
            "Train Epoch: 47 [19200/48000 (40%)]\tLoss: 0.000875\n",
            "Train Epoch: 47 [25600/48000 (53%)]\tLoss: 0.001407\n",
            "Train Epoch: 47 [32000/48000 (67%)]\tLoss: 0.003300\n",
            "Train Epoch: 47 [38400/48000 (80%)]\tLoss: 0.006947\n",
            "Train Epoch: 47 [44800/48000 (93%)]\tLoss: 0.001017\n",
            "\n",
            "Validation set: Average loss: 0.0835, Accuracy: 11733/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 48 [0/48000 (0%)]\tLoss: 0.000387\n",
            "Train Epoch: 48 [6400/48000 (13%)]\tLoss: 0.003238\n",
            "Train Epoch: 48 [12800/48000 (27%)]\tLoss: 0.002347\n",
            "Train Epoch: 48 [19200/48000 (40%)]\tLoss: 0.002239\n",
            "Train Epoch: 48 [25600/48000 (53%)]\tLoss: 0.006307\n",
            "Train Epoch: 48 [32000/48000 (67%)]\tLoss: 0.001328\n",
            "Train Epoch: 48 [38400/48000 (80%)]\tLoss: 0.004781\n",
            "Train Epoch: 48 [44800/48000 (93%)]\tLoss: 0.010019\n",
            "\n",
            "Validation set: Average loss: 0.0829, Accuracy: 11741/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 49 [0/48000 (0%)]\tLoss: 0.015247\n",
            "Train Epoch: 49 [6400/48000 (13%)]\tLoss: 0.000755\n",
            "Train Epoch: 49 [12800/48000 (27%)]\tLoss: 0.006035\n",
            "Train Epoch: 49 [19200/48000 (40%)]\tLoss: 0.002346\n",
            "Train Epoch: 49 [25600/48000 (53%)]\tLoss: 0.003801\n",
            "Train Epoch: 49 [32000/48000 (67%)]\tLoss: 0.000329\n",
            "Train Epoch: 49 [38400/48000 (80%)]\tLoss: 0.010365\n",
            "Train Epoch: 49 [44800/48000 (93%)]\tLoss: 0.005609\n",
            "\n",
            "Validation set: Average loss: 0.0835, Accuracy: 11744/12000 (98%)\n",
            "\n",
            "torch.Size([32]) torch.Size([32, 10])\n",
            "Train Epoch: 50 [0/48000 (0%)]\tLoss: 0.002128\n",
            "Train Epoch: 50 [6400/48000 (13%)]\tLoss: 0.002732\n",
            "Train Epoch: 50 [12800/48000 (27%)]\tLoss: 0.003743\n",
            "Train Epoch: 50 [19200/48000 (40%)]\tLoss: 0.002219\n",
            "Train Epoch: 50 [25600/48000 (53%)]\tLoss: 0.003094\n",
            "Train Epoch: 50 [32000/48000 (67%)]\tLoss: 0.000494\n",
            "Train Epoch: 50 [38400/48000 (80%)]\tLoss: 0.000831\n",
            "Train Epoch: 50 [44800/48000 (93%)]\tLoss: 0.008079\n",
            "\n",
            "Validation set: Average loss: 0.0842, Accuracy: 11740/12000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0796, Accuracy: 9788/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*학습된 모델을 바탕으로 각 사진 별 예측을 진행 해봅시다.*"
      ],
      "metadata": {
        "id": "kS-ouOru6ha_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch, label = next(iter(train_loader))\n",
        "\n",
        "image_idx = 5\n",
        "\n",
        "img = image_batch[image_idx, 0, :, :]\n",
        "plt.imshow(img, 'gray')\n",
        "plt.show()\n",
        "img = img.to(device)\n",
        "output = model(img)\n",
        "pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "print(\"Predict: \", pred[0, 0])\n",
        "print(\"Correct: \", label[image_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "b4e0a755-10be-4af4-f59e-bc6195cc8b29",
        "id": "Z4Rl-zJ-6ha_"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict:  tensor(4)\n",
            "Correct:  tensor(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbpIkxSL_WCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}